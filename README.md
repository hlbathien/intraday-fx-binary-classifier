# Intraday FX Binary Classifier

Deterministic, reproducible intraday foreign exchange (FX) modeling pipeline for predicting shortâ€‘horizon (1,5,10,15 minute) directional moves during the New York session and estimating move magnitude. The project produces both machineâ€‘learning metrics (ROC AUC, Brier, calibration) and trading KPIs (expected value per trade, gross return) under disciplined timeâ€‘series hygiene (walkâ€‘forward splits with embargo, no leakage, strictly pastâ€‘only features, topâ€‘ofâ€‘hour sequence predictions).

---
## âœ¨ Key Features
- Full endâ€‘toâ€‘end 5â€‘step pipeline (raw â†’ cleaned â†’ features â†’ sequences â†’ model training â†’ ranking)
- NY session filtering (12:00â€“20:59 America/New_York) with controlled smallâ€‘gap forward fill
- Multiâ€‘horizon targets: future kâ€‘minute log return sign & volatilityâ€‘scaled magnitude
- Vectorized feature engineering (returns, volatility, range, volume, timeâ€‘ofâ€‘day, zâ€‘scores)
- Hourly sequence extraction: 60 contiguous 1â€‘minute windows ending at topâ€‘ofâ€‘hour
- Dual modality modeling:
  - Tabular summarizations of sequences (statistical aggregates)
  - Joint sequence deep nets (GRU/LSTM/Transformer backbones) predicting sign (classification) + absolute magnitude (regression) simultaneously
- Probability calibration (Isotonic) with accuracyâ€‘oriented threshold search plus EVâ€‘driven secondary thresholds
- Walkâ€‘forward evaluation blocks (train / val / test) with embargo to reduce leakage
- Deterministic seeding across Python / NumPy / Torch
- Ranking engine computing composite model score & exports consolidated artifacts
- All paths centralized in `.config` (supports `@/` repoâ€‘relative resolution)

---
## ğŸ—‚ Directory Structure (core)
```
lib/                Reusable utilities (config, logging, randomness, modeling)
src/                Step scripts (1â€“5)
data/raw            Raw minute CSVs (Time, Open, High, Low, Close, Volume)
data/cleaned        NYâ€‘session filtered + small gap filled
/data/featured/<k>m Parquet feature + target tables per horizon
/data/sequenced     Sequence tensors (.npz) + walkâ€‘forward splits.json
/models/<SYMBOL>/   Trained artifacts per horizon + metrics
/models/rankings/   Aggregated ranking outputs
```

---
## âš™ï¸ Configuration
Edit `.config` at project root (example):
```
RAW_DATA_PATH=@/data/raw
CLEANED_DATA_PATH=@/data/cleaned
FEATURED_DATA_PATH=@/data/featured
SEQUENCED_DATA_PATH=@/data/sequenced
MODEL_OUTPUT=@/models
```
All scripts resolve these via `lib.config.load_config()`. Paths are created if missing.

---
## ğŸ“¦ Dependencies
Pinned in `requirements.txt` (Python â‰¥3.12 recommended):
- pandas, numpy, pyarrow / fastparquet
- scikit-learn, xgboost (optional if installed), torch
- tqdm, tabulate
Install:
```bash
pip install -r requirements.txt
```

---
## ğŸš€ Pipeline Steps
Run from project root (or `src/`). Each step is idempotent.

1. Data Preprocessing (`src/1_data_preprocessing.py`)
   - Reads raw CSVs â†’ filters NY session â†’ fills small (â‰¤5m) intraâ€‘session gaps â†’ writes cleaned CSV.

2. Feature Calculation (`src/2_features_calculation.py`)
   - Builds horizonâ€‘specific parquet datasets with pastâ€‘only engineered features and future targets for horizons {1,5,10,15}.

3. Sequencing (`src/3_sequencing.py`)
   - Extracts strictly contiguous 60â€‘minute windows ending at each topâ€‘ofâ€‘hour; reconstructs perâ€‘minute return `r1`; packs sequences + multiâ€‘horizon targets into compressed `.npz` per symbol; generates walkâ€‘forward `splits.json` (train/val/test blocks with embargo).

4. Model Training (`src/4_model_training.py`)
   - For each symbol & horizon: creates tabular summary features, builds candidate models (logistic L1, XGBoost variants, GRU/LSTM/Transformer specs), trains with early stopping, calibrates probabilities, optimizes accuracy threshold (0.45â€“0.65 search), derives EV trade filter theta, evaluates on test fold(s), saves metrics + model weights/state.

5. Results Aggregation / Ranking (`src/5_test_results.py`)
   - Aggregates `agg_metrics.json` across symbols/horizons; computes composite minâ€‘max weighted score; exports parquet, CSV rankings, best per horizon, top N JSON, and Markdown summary.

---
## ğŸ§ª Walkâ€‘Forward Splits
Defined in sequencing step:
- Train: 60 days
- Validation: 7 days (after 60â€‘minute embargo)
- Test: 21 days (after embargo)
- Slide forward by test block length; all folds processed independently.

This preserves chronological integrity and reduces leakage.

---
## ğŸ§  Modeling Details
Sequence models: joint network produces sign logit + nonâ€‘negative magnitude head. Training loss = 0.6 * BCE(logits, sign) + 0.4 * L1(magnitude, |target|) with optional label smoothing & class imbalance weighting. Optional mixed precision (fp16/bf16) and `torch.compile` support.

Tabular models: logistic regression (L1) and XGBoost (small/medium) when XGBoost is installed.

Probability calibration: Isotonic (fallback identity if insufficient validation points).

Threshold selection:
- Accuracy threshold search (probability) across [0.45, 0.65] increments 0.01.
- EV threshold search (p* & magnitude theta) retained for trade filtering and reporting.

Trading sizing: capped Kelly fraction (max fraction f_max=0.02) applied to selected trades.

---
## ğŸ“Š Metrics
Per fold (test set):
- Classification: roc_auc, average precision (if available), Brier score, expected calibration error (ECE), accuracy @0.5
- Trading: n_trades, ev_per_trade, total_ev, gross_return, gross_return_per_trade
- Thresholds: probability p_star (accuracy driven), magnitude theta (from EV search)

Aggregated (`agg_metrics.json`): mean across folds + fold count.

Ranking composite weights (see `5_test_results.py`):
```
ev_per_trade: 0.30
total_ev: 0.20
gross_return_per_trade: 0.15
gross_return: 0.10
roc_auc: 0.15
brier: -0.05 (lower better)
ece: -0.05 (lower better)
```

---
## ğŸ›  Usage Quickstart
```bash
# 1. Prepare raw CSVs in data/raw (Time,Open,High,Low,Close,Volume)
python src/1_data_preprocessing.py

# 2. Generate features
python src/2_features_calculation.py

# 3. Build sequences & splits
python src/3_sequencing.py

# 4. Train models
python src/4_model_training.py

# 5. Aggregate & rank results
python src/5_test_results.py
```
Outputs land in `models/` and `models/rankings/`.

---
## ğŸ§© Extending
- Add new engineered features: modify `2_features_calculation.py` ensuring pastâ€‘only semantics.
- Add sequence backbone: extend `_build_net` in `lib/modeling.py` (reuse pattern) and update `build_models` spec mapping.
- Adjust walkâ€‘forward regime: edit constants in `3_sequencing.py`.
- Change thresholds search ranges: modify `optimize_accuracy_threshold` / `optimize_thresholds` logic in `lib/modeling.py`.

---
## ğŸ”’ Reproducibility
`lib/random.seed_all()` seeds Python, NumPy, Torch (deterministic flags) once per run. Config snapshots stored with each symbol under `models/<SYMBOL>/config_snapshot.json` including SHAâ€‘256 of `.config` content.

---
## âš ï¸ Notes & Assumptions
- No data lookâ€‘ahead: all features shifted to exclude information from or after prediction timestamp.
- Sequence sampling restricted to topâ€‘ofâ€‘hour to reduce temporal correlation.
- Magnitude model predicts absolute (volâ€‘scaled) move size; trade decision combines probability & magnitude thresholds.
- XGBoost models skipped gracefully if library not installed.

---
## ğŸ“œ License
(Insert project license here if applicable.)

---
## ğŸ¤ Contributions
Issues & pull requests welcome. Please keep additions deterministic, timeâ€‘aware, and reuse utilities in `lib/` (avoid duplicating logic).

---
## ğŸ§¾ Citation
If you use this project in research or a report, consider citing with the repository URL and commit hash.

---
Happy modeling.
